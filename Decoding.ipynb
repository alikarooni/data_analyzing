{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import modules.encoding as enc\n",
    "import modules.common as common\n",
    "import pyodbc\n",
    "import re\n",
    "import ast\n",
    "import encodings\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import csv\n",
    "\n",
    "# this is to have the ability to clear the screen to display the progress bar\n",
    "from IPython.display import display, clear_output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading HotelObservation file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_108391/400772228.py:14: FutureWarning: DataFrame.applymap has been deprecated. Use DataFrame.map instead.\n",
      "  HotelObservations = HotelObservations.applymap(lambda x: remove_white_spaces(str(x)))\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import pandas as pd\n",
    "\n",
    "BASE_PATH = './'\n",
    "def remove_white_spaces(text):\n",
    "    return re.sub(r'\\s+', ' ', text)\n",
    "\n",
    "DATA_COL_NAME = 'combined_text'\n",
    "LABEL_COL_NAME = 'HotelAccountNo'\n",
    "\n",
    "top_1000_HotelObservation_file = f'{BASE_PATH}hotel_observation_where_count_greater_than_50_main.csv'\n",
    "HotelObservations = pd.read_csv(top_1000_HotelObservation_file, delimiter='^', quoting=3, escapechar='\\\\')\n",
    "\n",
    "HotelObservations = HotelObservations.applymap(lambda x: remove_white_spaces(str(x)))\n",
    "HotelObservations.replace('nan', '', inplace=True)\n",
    "\n",
    "\n",
    "HotelObservations = HotelObservations[[DATA_COL_NAME, LABEL_COL_NAME]]\n",
    "\n",
    "HotelObservations[DATA_COL_NAME] = \\\n",
    "    HotelObservations[DATA_COL_NAME].str.replace('nan', '', regex=False)\n",
    "HotelObservations[DATA_COL_NAME] = \\\n",
    "    HotelObservations[DATA_COL_NAME].str.replace('^', '', regex=False)\n",
    "HotelObservations[DATA_COL_NAME] = \\\n",
    "    HotelObservations[DATA_COL_NAME].str.replace('\"', '', regex=False)\n",
    "\n",
    "HotelObservations = HotelObservations[ HotelObservations[DATA_COL_NAME].apply(lambda x: len( [i for i in x.split(' ') if len(i)>0]) >= 4)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "is_valid_word: checks if the word is valid. as long as it has digit, letter, or some other chars, it is valid \n",
    "\n",
    "extract_values: extract columns from an insertion query. whatever comes after insert into VALUES\n",
    "\n",
    "try_decoding: try to decode a non-valid word. something like: Ï¿½Ï¿½Ï¿½Ï¿½\n",
    "\n",
    "try_encoding2: does the same, instead it works good in Hotels dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_encodings = ['ascii', 'base64_codec', 'big5', 'big5hkscs', 'bz2_codec', 'cp037', 'cp1026', 'cp1125', 'cp1140', 'cp1250', 'cp1251', 'cp1252', 'cp1253', 'cp1254', 'cp1255', 'cp1256', 'cp1257', 'cp1258', 'cp273', 'cp424', 'cp437', 'cp500', 'cp775', 'cp850', 'cp852', 'cp855', 'cp857', 'cp858', 'cp860', 'cp861', 'cp862', 'cp863', 'cp864', 'cp865', 'cp866', 'cp869', 'cp932', 'cp949', 'cp950', 'euc_jis_2004', 'euc_jisx0213', 'euc_jp', 'euc_kr', 'gb18030', 'gb2312', 'gbk', 'hex_codec', 'hp_roman8', 'hz', 'iso2022_jp', 'iso2022_jp_1', 'iso2022_jp_2', 'iso2022_jp_2004', 'iso2022_jp_3', 'iso2022_jp_ext', 'iso2022_kr', 'iso8859_10', 'iso8859_11', 'iso8859_13', 'iso8859_14', 'iso8859_15', 'iso8859_16', 'iso8859_2', 'iso8859_3', 'iso8859_4', 'iso8859_5', 'iso8859_6', 'iso8859_7', 'iso8859_8', 'iso8859_9', 'johab', 'koi8_r', 'kz1048', 'latin_1', 'mac_cyrillic', 'mac_greek', 'mac_iceland', 'mac_latin2', 'mac_roman', 'mac_turkish', 'mbcs', 'ptcp154', 'quopri_codec', 'rot_13', 'shift_jis', 'shift_jis_2004', 'shift_jisx0213', 'tactis', 'tis_620', 'utf_16', 'utf_16_be', 'utf_16_le', 'utf_32', 'utf_32_be', 'utf_32_le', 'utf_7', 'utf_8', 'uu_codec', 'zlib_codec']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dict_sort(d, sort_on='keys', desc=False):\n",
    "    if(sort_on == 'keys'):\n",
    "        return {k: v for k, v in sorted(d.items(), key=lambda item: item[0], reverse=desc)}\n",
    "    else:\n",
    "        return {k: v for k, v in sorted(d.items(), key=lambda item: item[1], reverse=desc)}\n",
    "\n",
    "def get_char_set(text):\n",
    "    char_set = set()\n",
    "    for i in text:\n",
    "        char_set.add(i)\n",
    "    return char_set\n",
    "\n",
    "def is_valid_word(word):\n",
    "    char_set = get_char_set(str(word))\n",
    "    char_set = [c for c in char_set if c != ' ']\n",
    "    for s in char_set:\n",
    "        if(not (s.isdigit() or s.isalpha() or s in ['_', '-', '+', '*', ' ', '·', '.', '(' , ')', \"\\x04\", '//', ',', '&', '@', '/', ';', ':', '=', '\\\\', '\t', ' ', '«',    \\\n",
    "                                                    '|', '!', '$', '#', '?', '%', \"'\", '\"', '`', '’', '´', ' ', '–', '[', ']', '(', ')', '~', '<', '>', '°', '®', '©'   ])):\n",
    "            return False\n",
    "    return True\n",
    "\n",
    "def is_valid_data_row(sentence):\n",
    "    for word in sentence:\n",
    "        if(not is_valid_word(word)):\n",
    "            return False\n",
    "    return True\n",
    "\n",
    "def get_valid_word(s):\n",
    "    word  = ''\n",
    "    for s in str(s):\n",
    "        if(s.isdigit() or s.isalpha() or s == ' '):\n",
    "            word += s\n",
    "    return word\n",
    "\n",
    "def extract_values(text):\n",
    "    values = []\n",
    "    is_recording = False\n",
    "    value = ''\n",
    "\n",
    "    for char in text:\n",
    "        if char == \"'\" and not is_recording:  # Start recording\n",
    "            is_recording = True\n",
    "            continue\n",
    "        if char == \"'\" and is_recording:  # Stop recording\n",
    "            is_recording = False\n",
    "            values.append(value)\n",
    "            value = ''\n",
    "            continue\n",
    "        if is_recording:\n",
    "            value += char\n",
    "\n",
    "    return values\n",
    "\n",
    "def try_decoding(s):\n",
    "    potential_originals = []\n",
    "    for encoding in all_encodings:\n",
    "        try:\n",
    "            potential_original = s.encode(encoding).decode('utf-8')\n",
    "            if(is_valid_word(potential_original)):\n",
    "                potential_originals.append((encoding, potential_original))\n",
    "        except:\n",
    "            continue\n",
    "    return potential_originals#[0] if(len(potential_originals) >=1) else ''\n",
    "\n",
    "def try_decoding2(s):\n",
    "    potential_originals = {}\n",
    "    for encoding in all_encodings:\n",
    "        try:\n",
    "            potential_original = s.encode('iso8859_1').decode(encoding)\n",
    "            if(is_valid_word(potential_original)):\n",
    "                potential_originals[encoding] = potential_original\n",
    "        except:\n",
    "            continue\n",
    "    return potential_originals\n",
    "\n",
    "def try_decoding3(s):\n",
    "    potential_originals = []\n",
    "    for encoding1 in all_encodings:\n",
    "        for encoding2 in all_encodings:\n",
    "            try:\n",
    "                potential_original = s.encode(encoding1).decode(encoding2)\n",
    "                if(is_valid_word(potential_original)):\n",
    "                    potential_originals.append((encoding1, encoding2, potential_original))\n",
    "            except:\n",
    "                continue\n",
    "    return potential_originals\n",
    "\n",
    "get_all_countries = \\\n",
    "[\n",
    "    \"Afghanistan\", \"Albania\", \"Algeria\", \"Andorra\", \"Angola\", \"Antigua and Barbuda\", \"Barbuda\", \"Antigua\" \"Argentina\", \"argentina\",\n",
    "    \"Armenia\", \"Australia\", \"Austria\", \"™sterreich\", \"sterrike\", \"Azerbaijan\", \"Bahamas\", \"Bahrain\", \"Bangladesh\", \"Barbados\", \n",
    "    \"Belarus\", \"Belgium\", \"Belize\", \"Benin\", \"Bhutan\", \"Bolivia\", \"Bosnia and Herzegovina\", \"Botswana\", \n",
    "    \"Brazil\", \"Brunei\", \"Bulgaria\", \"Burkina Faso\", \"Burundi\", \"Côte d'Ivoire\", \"Cabo Verde\", \"cape verde\",\n",
    "    \"Cambodia\", \"Cameroon\", \"Canada\", \"Central African Republic\", \"Chad\", \"Chile\", \"China\", \n",
    "    \"Colombia\", \"Comoros\", \"Congo\", \"Congo Brazzaville\", \"Brazzaville\", \"Costa Rica\", \"Croatia\", \"Cuba\", \"Cyprus\", \n",
    "    \"Czechia\", \"Czech Republic\", \"Democratic Republic of the Congo\", \"Denmark\", \"danmark\", \"denemarken\", \"džnemark\", \"faroe islands\", \"greenland\", \"Djibouti\", \"Dominica\", \n",
    "    \"Dominican Republic\", \"Ecuador\", \"Egypt\", \"El Salvador\", \"Equatorial Guinea\", \"Eritrea\", \"Estonia\", \n",
    "    \"Eswatini\", \"Swaziland\" \"Ethiopia\", \"ethiopia\", \"Fiji\", \"Finland\", \"France\", \"Frankreich\", \"frankrijk\", \"Gabon\", \"Gambia\", \"Georgia\", \n",
    "    \"Germany\", \"Duitsland\", \"Deutschland\", \"geemany\", \"Ghana\", \"Greece\", \"Grenada\", \"Guatemala\", \"Guinea\", \"Guinea Bissau\", \"Bissau\" \"Guyana\", \"Haiti\", \n",
    "    \"Holy See\", \"Honduras\", \"Hong Kong\", \"hongkong\", \"Hungary\", \"Ungarn\", \"Iceland\", \"India\", \"Indonesia\", \"Iran\", \"Iraq\", \"Ireland\", \n",
    "    \"Israel\", \"Italy\", \"Ivory\", \"d'ivoire \", \"Jamaica\", \"Japan\", \"Jordan\", \"Kazakhstan\", \"Kenya\", \"Kiribati\", \"Kuwait\", \n",
    "    \"Kyrgyzstan\", \"Laos\", \"Latvia\", \"Lebanon\", \"Lesotho\", \"Liberia\", \"Libya\", \"Liechtenstein\", \n",
    "    \"Lithuania\", \"Luxembourg\", \"Madagascar\", \"Malawi\", \"Malaysia\", \"Maldives\", \"Mali\", \"Malta\", \n",
    "    \"Marshall Islands\", \"Mauritania\", \"Mauritius\", \"Mexico\", \"Micronesia\", \"Moldova\", \"Monaco\", \n",
    "    \"Mongolia\", \"Montenegro\", \"Morocco\", \"Mozambique\", \"Myanmar\", \"Burma\" \"Namibia\", \"Nauru\", \n",
    "    \"Nepal\", \"Netherlands\", \"nederland\", \"New Zealand\", \"Nicaragua\", \"Niger\", \"Nigeria\", \"North Korea\", \"Korea\",\n",
    "    \"North Macedonia\", \"Macedonia\", \"Norway\", \"norge\" \"Oman\", \"Pakistan\", \"Palau\", \"Palestine State\", \n",
    "    \"Panama\", \"Papua New Guinea\", \"Paraguay\", \"Peru\", \"Philippines\", \"phillippines\", \"Poland\", \"Portugal\", \n",
    "    \"Qatar\", \"Romania\", \"Russia\", \"Rwanda\", \"Saint Kitts and Nevis\", \"Saint Lucia\", \n",
    "    \"Saint Vincent and the Grenadines\", \"Samoa\", \"San Marino\", \"Sao Tome and Principe\", \"Saudi Arabia\", \n",
    "    \"Senegal\", \"Serbia\", \"Seychelles\", \"Sierra Leone\", \"Singapore\", \"Slovakia\", \"Slovenia\", \n",
    "    \"Solomon Islands\", \"Somalia\", \"South Africa\", \"South Korea\", \"South Sudan\", \"Spain\", \"espaã‘a\", \"spanien\", \"spanje\", \"Sri Lanka\", \n",
    "    \"Sudan\", \"Suriname\", \"Sweden\", \"Sverige\", \"Switzerland\", \"schweiz\", \"zwitserland\", \"Syria\", \"Taiwan\", \"Tajikistan\", \"Tanzania\", \"Thailand\", \n",
    "    \"Timor Leste\", \"Togo\", \"Tonga\", \"Trinidad and Tobago\", \"Tunisia\", \"Turkey\", \"Turkmenistan\", \n",
    "    \"Tuvalu\", \"Uganda\", \"Ukraine\", \"United Arab Emirates\", \"United Kingdom\", \"england\", \"United States of America\", \"United State\",\n",
    "    \"USA\", \"Uruguay\", \"Uzbekistan\", \"Vanuatu\", \"Venezuela\", \"ver.k™nigreich\", \"Vietnam\", \"Yemen\", \"Zambia\", \"Zimbabwe\"\n",
    "]\n",
    "\n",
    "def similar_name_places():\n",
    "    return [\n",
    "    #city_name, alternative_country, Origin_country\n",
    "    (\"Paris\", \"USA\", \"France\"),\n",
    "    (\"Jordan\", \"USA\", \"Jordan\"),\n",
    "    (\"Georgia\", \"USA\", \"Georgia\"),\n",
    "    (\"Lebanon\", \"USA\", \"Lebanon\"),\n",
    "    (\"Norway\", \"USA\", \"Norway\"),\n",
    "    (\"Panama\", \"USA\", \"Panama\"),\n",
    "    (\"Belgium\", \"USA\", \"Belgium\"),\n",
    "    (\"Moscow\", \"USA\", \"Russia\"),\n",
    "    (\"Peru\", \"USA\", \"Peru\"),\n",
    "    (\"Bolivia\", \"North Carolina, USA\", \"Bolivia\"),\n",
    "    (\"Jamaica\", \"New York, USA\", \"Jamaica\"),\n",
    "    (\"Luxembourg\", \"Wisconsin, USA\", \"Luxembourg\"),\n",
    "    (\"Mexico\", \"Missouri, USA\", \"Mexico\"),\n",
    "    (\"Sydney\", \"Montana, USA\", \"Australia\"),\n",
    "    (\"Venezuela\", \"Colombia\", \"Venezuela\"),\n",
    "    (\"Cuba\", \"Missouri, USA\", \"Cuba\"),\n",
    "    (\"Barbados\", \"Ontario, Canada\", \"Barbados\"),\n",
    "    (\"Cambridge\", \"New Zealand\", \"United Kingdom\"),\n",
    "    (\"Egypt\", \"Texas, USA\", \"Egypt\"),\n",
    "    (\"Denmark\", \"South Carolina, USA\", \"Denmark\"),\n",
    "    (\"Wellington\", \"Tamil Nadu, India\", \"New Zealand\"),\n",
    "    (\"Greece\", \"New York, USA\", \"Greece\"),\n",
    "    (\"Malta\", \"Montana, USA\", \"Malta\"),\n",
    "    (\"India\", \"Missouri, USA\", \"India\"),\n",
    "    (\"Ireland\", \"Indiana, USA\", \"Ireland\")\n",
    "]\n",
    "\n",
    "get_letter_collections = \\\n",
    "    {\n",
    "        \"english\": set(['A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z']),\n",
    "        \"finland\": set(['Ä', 'Ö']),\n",
    "        \"denmark\": set(['Æ', 'Ø', 'Å']),\n",
    "        \"norway\": set(['Æ', 'Ø', 'Å']),\n",
    "        \"sweden\": set(['Å', 'Ä', 'Ö']),\n",
    "        \"france\": set(['À', 'Â', 'Æ', 'Ç', 'É', 'È', 'Ê', 'Ë', 'Î', 'Ï', 'Ô', 'Œ', 'Ù', 'Û', 'Ü', 'Ÿ' ]),        \n",
    "        \"czech republic\": set(['Á', 'Č', 'Ď', 'É', 'Ě', 'Í', 'Ň', 'Ó', 'Ř', 'Š', 'Ť', 'Ú', 'Ů', 'Ý', 'Ž']),\n",
    "        \"spain\": set(['Ñ']),\n",
    "        \"slovene\": set(['Č', 'Š', 'Ž']),\n",
    "        \"croatian\": set(['Č', 'Ć', 'Đ', 'Š', 'Ž']),\n",
    "        \"hungarian\": set(['Á', 'É', 'Í', 'Ó', 'Ö', 'Ő', 'Ú', 'Ü', 'Ű']),\n",
    "        \"romanian\": set(['Ă', 'Â', 'Î', 'Ș', 'Ț', ]), \n",
    "\n",
    "        \"russia\": set(['А', 'Б', 'В', 'Г', 'Д', 'Е', 'Ё', 'Ж', 'З', 'И', 'Й', 'К', 'Л', 'М', 'Н', 'О', 'П', 'Р', 'С', 'Т', 'У', 'Ф', 'Х', 'Ц', 'Ч', 'Ш', 'Щ', 'Ъ', 'Ы', 'Ь', 'Э', 'Ю', 'Я']),\n",
    "        \"japan_1\":set(['あ', 'い', 'う', 'え', 'お', 'か', 'き', 'く', 'け', 'こ', 'さ', 'し', 'す', 'せ', 'そ', 'た', 'ち', 'つ', 'て', 'と', 'な', 'に', 'ぬ', 'ね', 'の', 'は', 'ひ', 'ふ', 'へ', \n",
    "            'ほ', 'ま', 'み', 'む', 'め', 'も', 'や', 'ゆ', 'よ', 'ら', 'り', 'る', 'れ', 'ろ', 'わ', 'を', 'ん']),\n",
    "        \"japan_2\": set(['ア', 'イ', 'ウ', 'エ', 'オ', 'カ', 'キ', 'ク', 'ケ', 'コ', 'サ', 'シ', 'ス', 'セ', 'ソ', 'タ', 'チ', 'ツ', 'テ', 'ト', 'ナ', 'ニ', 'ヌ', 'ネ', 'ノ', 'ハ', 'ヒ', 'フ', 'ヘ', \n",
    "            'ホ', 'マ', 'ミ', 'ム', 'メ', 'モ', 'ヤ', 'ユ', 'ヨ', 'ラ', 'リ', 'ル', 'レ', 'ロ', 'ワ', 'ヲ', 'ン']),\\\n",
    "        \"thailand\": set(['ก', 'ข', 'ฃ', 'ค', 'ฅ', 'ฆ', 'ง', 'จ', 'ฉ', 'ช', 'ซ', 'ฌ', 'ญ', 'ฎ', 'ฏ', 'ฐ', 'ฑ', 'ฒ', 'ณ', 'ด', 'ต', 'ถ', 'ท', 'ธ', 'น', 'บ', 'ป', 'ผ', 'ฝ', 'พ', 'ฟ', 'ภ', \n",
    "            'ม', 'ย', 'ร', 'ฤ', 'ล', 'ฦ', 'ว', 'ศ', 'ษ', 'ส', 'ห', 'ฬ', 'อ', 'ฮ']),\n",
    "    }\n",
    "\n",
    "\n",
    "alternative_chars = [\n",
    "        (set(['Ä', 'Æ', 'Å', 'Æ', 'Å', 'Å', 'Ä', 'À', 'Â', 'Æ', 'Á', 'Á', 'Ă', 'Â'  ]), 'A'),\n",
    "        (set(['Ç', 'Č', 'Č', 'Č', 'Ć' ]), 'C'),\n",
    "        (set(['Đ', 'Ď' ]), 'D'),\n",
    "        (set(['É', 'È', 'Ê', 'Ë', 'É', 'Ě', 'É' ]), 'E'),\n",
    "        (set(['Î', 'Ï', 'Í', 'Î', 'Í' ]), 'I'),\n",
    "        (set(['Ñ', 'Ň' ]), 'N'),\n",
    "        (set(['Ö', 'Ø', 'Ø', 'Ö', 'Ô', 'Ó', 'Ó', 'Ö', 'Ő', 'Œ'  ]), 'O'),\n",
    "        (set(['Ř' ]), 'R'),\n",
    "        (set(['Š', 'Š', 'Ș', 'Š' ]), 'S'),\n",
    "        (set(['Ț', 'Ť' ]), 'T'),\n",
    "        (set(['Ù', 'Û', 'Ü', 'Ú', 'Ů', 'Ú', 'Ü', 'Ű' ]), 'U'),\n",
    "        (set(['Ÿ', 'Ý' ]), 'Y'),\n",
    "        (set(['Ž', 'Ž', 'Ž' ]), 'Z')\n",
    "]\n",
    "        \n",
    "\n",
    "def is_chinese(word):\n",
    "    for character in word:\n",
    "        if not ('\\u4e00' <= character <= '\\u9fff'): return False\n",
    "    return True\n",
    "\n",
    "def is_japanese(word):\n",
    "    for character in word:\n",
    "        if ('\\u3040' <= character <= '\\u309F' or  # Hiragana\n",
    "            '\\u30A0' <= character <= '\\u30FF' or  # Katakana\n",
    "            '\\u4E00' <= character <= '\\u9FFF' or  # Common Kanji\n",
    "            '\\u3400' <= character <= '\\u4DBF' or  # Extended Kanji A\n",
    "            '\\uF900' <= character <= '\\uFAFF' or  # Extended Kanji B\n",
    "            '\\uFF66' <= character <= '\\uFF9F'):   # Katakana Phonetic Extensions\n",
    "            continue\n",
    "        else: return False\n",
    "    return True\n",
    "\n",
    "def is_russian(text):\n",
    "    for character in text:\n",
    "        if not ('\\u0400' <= character <= '\\u04FF' or  # Cyrillic\n",
    "                '\\u0500' <= character <= '\\u052F'):  # Cyrillic Supplement\n",
    "            return False\n",
    "    return True\n",
    "\n",
    "def is_thai(text):\n",
    "    for character in text:\n",
    "        if not ('\\u0E00' <= character <= '\\u0E7F'): return False\n",
    "    return True\n",
    "    \n",
    "def is_valid_word_by_chars(word):\n",
    "    word = word.upper()\n",
    "    word_length = len(word)    \n",
    "    english_letters = get_letter_collections['english']\n",
    "    finland_letters = get_letter_collections['finland']\n",
    "    denmark_letters = get_letter_collections['denmark']\n",
    "    norway_letters = get_letter_collections['norway']\n",
    "    sweden_letters = get_letter_collections['sweden']\n",
    "    france_letters = get_letter_collections['france']\n",
    "    czech_letters = get_letter_collections['czech republic']\n",
    "    spain_letters = get_letter_collections['spain']\n",
    "    slovene_letters = get_letter_collections['slovene']\n",
    "    croatian_letters = get_letter_collections['croatian']\n",
    "    hungarian_letters = get_letter_collections['hungarian']\n",
    "    romanian_letters = get_letter_collections['romanian']\n",
    "\n",
    "    valid_chars_counter = len([1 for w in word if w in english_letters]) \n",
    "    if(valid_chars_counter == word_length): return (True, 'english')\n",
    "\n",
    "    if((valid_chars_counter + len([1 for w in word if w in finland_letters])) == word_length): return (True, 'finland')\n",
    "    if((valid_chars_counter + len([1 for w in word if w in denmark_letters])) == word_length): return (True, 'denmark')\n",
    "    if((valid_chars_counter + len([1 for w in word if w in norway_letters])) == word_length): return (True, 'norway')\n",
    "    if((valid_chars_counter + len([1 for w in word if w in sweden_letters])) == word_length): return (True, 'sweden')\n",
    "    if((valid_chars_counter + len([1 for w in word if w in france_letters])) == word_length): return (True, 'france')\n",
    "    if((valid_chars_counter + len([1 for w in word if w in czech_letters])) == word_length): return (True, 'czech')\n",
    "    if((valid_chars_counter + len([1 for w in word if w in spain_letters])) == word_length): return (True, 'spain')\n",
    "    if((valid_chars_counter + len([1 for w in word if w in slovene_letters])) == word_length): return (True, 'slovene')\n",
    "    if((valid_chars_counter + len([1 for w in word if w in croatian_letters])) == word_length): return (True, 'croatian')\n",
    "    if((valid_chars_counter + len([1 for w in word if w in hungarian_letters])) == word_length): return (True, 'hungarian')\n",
    "    if((valid_chars_counter + len([1 for w in word if w in romanian_letters])) == word_length): return (True, 'romanian')\n",
    "\n",
    "    if(is_chinese(word)): return (True, 'china')\n",
    "    if(is_japanese(word)): return (True, 'japan')\n",
    "    if(is_russian(word)): return (True, 'russia')\n",
    "    if(is_thai(word)): return (True, 'thailand')\n",
    "\n",
    "    return (False, '')\n",
    "\n",
    "def replace_letter_in_word(word,i , l):\n",
    "    return word[:i] + l + word[i+1:]\n",
    "\n",
    "def replace_alternative_letter(word):\n",
    "    for index, l in enumerate(word):\n",
    "        for letters, alternative in alternative_chars:\n",
    "            if(l in letters): \n",
    "                word = replace_letter_in_word(word, index, alternative)\n",
    "    return word.lower()\n",
    "\n",
    "def remove_duplicated_letters(word):\n",
    "    res_word = ''\n",
    "    for i in range(len(word)):\n",
    "        if(i == 0):\n",
    "            res_word = word[i]; continue\n",
    "        if(word[i] == word[i-1]): continue\n",
    "        else: res_word += word[i]\n",
    "    return res_word\n",
    "\n",
    "def fine_tune_word(word):\n",
    "    word = word.upper()\n",
    "    word = replace_alternative_letter(word)\n",
    "    word = remove_duplicated_letters(word)\n",
    "    return word\n",
    "\n",
    "def list_valid_words(word):\n",
    "    valid_words = set()\n",
    "    for _, _, word in try_decoding3(word):\n",
    "        status, country = is_valid_word_by_chars(word)\n",
    "        if(status):\n",
    "            valid_words.add((country, fine_tune_word(word)))\n",
    "    return valid_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "{\n",
    "    \"Afghanistan\":\"\",\n",
    "    \"Albania\":\"\",\n",
    "    \"Algeria\":\"\", \"Andorra\":\"\", \"Angola\":\"\", \"Antigua and Barbuda\":\"\", \"Barbuda\":\"\", \"Antigua\":\"\", \"Argentina\":\"\", \"argentina\":\"\",\n",
    "    \"Armenia\":\"\", \"Australia\":\"\", \"Austria\":\"\", \"™sterreich\":\"\", \"sterrike\":\"\", \"Azerbaijan\":\"\", \"Bahamas\":\"\", \"Bahrain\":\"\", \"Bangladesh\":\"\", \"Barbados\":\"\", \n",
    "    \"Belarus\":\"\", \"Belgium\":\"\", \"Belize\":\"\", \"Benin\":\"\", \"Bhutan\":\"\", \"Bolivia\":\"\", \"Bosnia and Herzegovina\":\"\", \"Botswana\":\"\", \n",
    "    \"Brazil\":\"\", \"Brunei\":\"\", \"Bulgaria\":\"\", \"Burkina Faso\":\"\", \"Burundi\":\"\", \"Côte d'Ivoire\":\"\", \"Cabo Verde\":\"\", \"cape verde\":\"\",\n",
    "    \"Cambodia\":\"\", \"Cameroon\":\"\", \"Canada\":\"\", \"Central African Republic\":\"\", \"Chad\":\"\", \"Chile\":\"\", \"China\":\"\", \n",
    "    \"Colombia\":\"\", \"Comoros\":\"\", \"Congo\":\"\", \"Congo Brazzaville\":\"\", \"Brazzaville\":\"\", \"Costa Rica\":\"\", \"Croatia\":\"\", \"Cuba\":\"\", \"Cyprus\":\"\", \n",
    "    \"Czechia\":\"\", \"Czech Republic\":\"\", \"Democratic Republic of the Congo\":\"\", \"Denmark\":\"\", \"danmark\":\"\", \"denemarken\":\"\", \"džnemark\":\"\", \"faroe islands\":\"\", \"greenland\":\"\", \"Djibouti\":\"\", \"Dominica\":\"\", \n",
    "    \"Dominican Republic\":\"\", \"Ecuador\":\"\", \"Egypt\":\"\", \"El Salvador\":\"\", \"Equatorial Guinea\":\"\", \"Eritrea\":\"\", \"Estonia\":\"\", \n",
    "    \"Eswatini\":\"\", \"Swaziland\":\"\", \"Ethiopia\":\"\", \"ethiopia\":\"\", \"Fiji\":\"\", \"Finland\":\"\", \"France\":\"\", \"Frankreich\":\"\", \"frankrijk\":\"\", \"Gabon\":\"\", \"Gambia\":\"\", \"Georgia\":\"\", \n",
    "    \"Germany\":\"\", \"Duitsland\":\"\", \"Deutschland\":\"\", \"geemany\":\"\", \"Ghana\":\"\", \"Greece\":\"\", \"Grenada\":\"\", \"Guatemala\":\"\", \"Guinea\":\"\", \"Guinea Bissau\":\"\", \"Bissau\":\"\" \"Guyana\", \"Haiti\":\"\", \n",
    "    \"Holy See\":\"\", \"Honduras\":\"\", \"Hong Kong\":\"\", \"hongkong\":\"\", \"Hungary\":\"\", \"Ungarn\":\"\", \"Iceland\":\"\", \"India\":\"\", \"Indonesia\":\"\", \"Iran\":\"\", \"Iraq\":\"\", \"Ireland\":\"\", \n",
    "    \"Israel\":\"\", \"Italy\":\"\", \"Ivory\":\"\", \"d'ivoire\":\"\", \"Jamaica\":\"\", \"Japan\":\"\", \"Jordan\":\"\", \"Kazakhstan\":\"\", \"Kenya\":\"\", \"Kiribati\":\"\", \"Kuwait\":\"\", \n",
    "    \"Kyrgyzstan\":\"\", \"Laos\":\"\", \"Latvia\":\"\", \"Lebanon\":\"\", \"Lesotho\":\"\", \"Liberia\":\"\", \"Libya\":\"\", \"Liechtenstein\":\"\", \n",
    "    \"Lithuania\":\"\", \"Luxembourg\":\"\", \"Madagascar\":\"\", \"Malawi\":\"\", \"Malaysia\":\"\", \"Maldives\":\"\", \"Mali\":\"\", \"Malta\":\"\", \n",
    "    \"Marshall Islands\":\"\", \"Mauritania\":\"\", \"Mauritius\":\"\", \"Mexico\":\"\", \"Micronesia\":\"\", \"Moldova\":\"\", \"Monaco\":\"\", \n",
    "    \"Mongolia\":\"\", \"Montenegro\":\"\", \"Morocco\":\"\", \"Mozambique\":\"\", \"Myanmar\":\"\", \"Burma\":\"\", \"Namibia\":\"\", \"Nauru\":\"\", \n",
    "    \"Nepal\":\"\", \"Netherlands\":\"\", \"nederland\":\"\", \"New Zealand\":\"\", \"Nicaragua\":\"\", \"Niger\":\"\", \"Nigeria\":\"\", \"North Korea\":\"\", \"Korea\":\"\",\n",
    "    \"North Macedonia\":\"\", \"Macedonia\":\"\", \"Norway\":\"\", \"norge\":\"\", \"Oman\":\"\", \"Pakistan\":\"\", \"Palau\":\"\", \"Palestine State\":\"\", \n",
    "    \"Panama\":\"\", \"Papua New Guinea\":\"\", \"Paraguay\":\"\", \"Peru\":\"\", \"Philippines\":\"\", \"phillippines\":\"\", \"Poland\":\"\", \"Portugal\":\"\", \n",
    "    \"Qatar\":\"\", \"Romania\":\"\", \"Russia\":\"\", \"Rwanda\":\"\", \"Saint Kitts and Nevis\":\"\", \"Saint Lucia\":\"\", \n",
    "    \"Saint Vincent and the Grenadines\":\"\", \"Samoa\":\"\", \"San Marino\":\"\", \"Sao Tome and Principe\":\"\", \"Saudi Arabia\":\"\", \n",
    "    \"Senegal\":\"\", \"Serbia\":\"\", \"Seychelles\":\"\", \"Sierra Leone\":\"\", \"Singapore\":\"\", \"Slovakia\":\"\", \"Slovenia\":\"\", \n",
    "    \"Solomon Islands\":\"\", \"Somalia\":\"\", \"South Africa\":\"\", \"South Korea\":\"\", \"South Sudan\":\"\", \"Spain\":\"\", \"espaã‘a\":\"\", \"spanien\":\"\", \"spanje\":\"\", \"Sri Lanka\":\"\", \n",
    "    \"Sudan\":\"\", \"Suriname\":\"\", \"Sweden\":\"\", \"Sverige\":\"\", \"Switzerland\":\"\", \"schweiz\":\"\", \"zwitserland\":\"\", \"Syria\":\"\", \"Taiwan\":\"\", \"Tajikistan\":\"\", \"Tanzania\":\"\", \"Thailand\":\"\", \n",
    "    \"Timor Leste\":\"\", \"Togo\":\"\", \"Tonga\":\"\", \"Trinidad and Tobago\":\"\", \"Tunisia\":\"\", \"Turkey\":\"\", \"Turkmenistan\":\"\", \n",
    "    \"Tuvalu\":\"\", \"Uganda\":\"\", \"Ukraine\":\"\", \"United Arab Emirates\":\"\", \"United Kingdom\":\"\", \"england\":\"\", \"United States of America\":\"\", \"United State\":\"\",\n",
    "    \"USA\":\"\", \"Uruguay\":\"\", \"Uzbekistan\":\"\", \"Vanuatu\":\"\", \"Venezuela\":\"\", \"ver.k™nigreich\":\"\", \"Vietnam\":\"\", \"Yemen\":\"\", \"Zambia\":\"\", \"Zimbabwe\":\"\"\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "clean up data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'25510000 / 25518392'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "texts = HotelObservations[DATA_COL_NAME].values\n",
    "remove_index = []\n",
    "\n",
    "index = 0\n",
    "df_length = len(HotelObservations)\n",
    "\n",
    "for i in range(len(texts)):    \n",
    "    new_text = []\n",
    "    for j, word in enumerate(texts[i].split(' ')):\n",
    "         if (is_valid_word(word)):\n",
    "            new_text.append(word)\n",
    "            remove_index.append(j)\n",
    "    texts[i] = ' '.join(new_text)\n",
    "\n",
    "    index += 1\n",
    "    if(index % 10000 == 0):\n",
    "        clear_output(wait=True)\n",
    "        display(f'{index} / {df_length}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "count the number of rows with bad words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts = HotelObservations[DATA_COL_NAME].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'counter:0, 25510000 / 25518392'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "counter = 0\n",
    "df_length = len(HotelObservations)\n",
    "index = 0\n",
    "for text in texts:\n",
    "    for word in text.split(' '):\n",
    "         if (not is_valid_word(word)):\n",
    "             counter += 1\n",
    "             break\n",
    "    \n",
    "    index += 1\n",
    "    if(index % 10000 == 0):\n",
    "        clear_output(wait=True)\n",
    "        display(f'counter:{counter}, {index} / {df_length}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "keeping a list or rows with bad words grup by their countries. This make is easier and faster to find good solution for converting bad words into right unicode."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rows_with_bad_words_by_country = {}\n",
    "rows_with_bad_words_by_country['other'] = set()\n",
    "\n",
    "index = 0\n",
    "length = len(HotelObservations)\n",
    "for i, row in HotelObservations.iterrows():\n",
    "    text = row[DATA_COL_NAME].lower()\n",
    "    bad_words = [word for word in text.split(' ') if not is_valid_word(word)]\n",
    "    if (len(bad_words) == 0): continue\n",
    "    country_in_text = False\n",
    "    for country in get_all_countries:\n",
    "        cnty = country.lower()\n",
    "        if(cnty in text):\n",
    "            if(rows_with_bad_words_by_country.get(cnty, None) == None): rows_with_bad_words_by_country[cnty] = set()\n",
    "            for b_word in bad_words:\n",
    "                rows_with_bad_words_by_country[cnty].add(b_word)\n",
    "            country_in_text = True\n",
    "            break\n",
    "    if(not country_in_text):\n",
    "        for b_word in bad_words:\n",
    "            rows_with_bad_words_by_country['other'].add(b_word)\n",
    "    clear_output(wait=True)\n",
    "    index+=1\n",
    "    display(f'iterating {index + 1} / {length}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for word in rows_with_bad_words_by_country['denmark']:\n",
    "#     if(common.is_all_digit(word)): continue\n",
    "#     list_valid_words(word)\n",
    "#     print('-----------')\n",
    "\n",
    "rows_with_bad_words_by_country['denmark']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_valid_words('poissonniã¨re')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "analysing rows_with_bad_words_by_country"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_countries_with_bad_words = {}\n",
    "for k, v in rows_with_bad_words_by_country.items():\n",
    "    count_countries_with_bad_words [k] = len(v)\n",
    "\n",
    "print(len(count_countries_with_bad_words))\n",
    "# count_countries_with_bad_words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "trying to find corrected word for bad_words by language_letter's collection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_correct_word_by_language(word, language_letters):\n",
    "    for l in set(word):\n",
    "        if(l.upper() in language_letters): continue\n",
    "        else: return False\n",
    "    return True\n",
    "\n",
    "def find_correct_word(word, language_letters):\n",
    "    res = set()\n",
    "    decoded_words = [decoded_word for _, _, decoded_word in try_decoding3(word)]\n",
    "    for dec_word in decoded_words:\n",
    "        if(find_correct_word_by_language(dec_word, language_letters)):\n",
    "            res.add(dec_word)\n",
    "    return res\n",
    "\n",
    "index = 0\n",
    "bad_word_lookup = {}\n",
    "for country, texts in rows_with_bad_words_by_country.items():\n",
    "    # if(country.lower() == 'other'): continue\n",
    "    display(f'country: {country} - {index + 1} / {len(rows_with_bad_words_by_country)} - number of texts in {country}={len(texts)}')\n",
    "    for text in texts:\n",
    "        for word in text.split(' '):\n",
    "            if(is_valid_word(word)): continue\n",
    "            bad_word_lookup[word] = {}\n",
    "            for language, language_letters in get_letter_collections.items():\n",
    "                bad_word_lookup[word][language] = find_correct_word(word, language_letters)\n",
    "\n",
    "    clear_output(wait=True)\n",
    "    index += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "file = open('rows_with_bad_words_by_country.bin', 'w')\n",
    "pickle.dump(rows_with_bad_words_by_country, file)\n",
    "\n",
    "file = open('bad_word_lookup.bin', 'w')\n",
    "pickle.dump(bad_word_lookup, file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log = ''\n",
    "for k, v in bad_word_lookup.items():\n",
    "    log += f'for bad_word {k} values are: '\n",
    "    any_value = False\n",
    "    for language, corrected_word in v.items():\n",
    "        log += f'language {k}: {\", \".join(v)}'\n",
    "        if (len(v) > 0): any_value=True\n",
    "\n",
    "    if(any_value): print(log)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_valid_words('¬ESK BUD·JOVICE')\n",
    "\n",
    "\n",
    "# print('----')\n",
    "# try_decoding3('hiller™d')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for language, letters in get_letter_collections.items():\n",
    "    print(language, find_correct_word('chã¢tre', letters))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking for rows where they have exceptional country city combination. EX: USA and Paris."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, row in HotelObservations.iterrows():\n",
    "    text = row[DATA_COL_NAME].lower()\n",
    "    if (len([word for word in text if not is_valid_word(word)]) == 0): continue\n",
    "    found = False\n",
    "    for country in get_all_countries():\n",
    "        if(country.lower() in text.lower()):\n",
    "            found = True; break\n",
    "    if(not found):\n",
    "        print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "replacements = {\n",
    "                'Norway': {'Ï¿½':'Ø', '¥':'Ø'},\n",
    "                'Germany': {'Ã?Â¶':'ö', 'Ã?Â¼':'ü', 'ÃƒÅ¸':'ß', '¥':'ö', '':'ü', 'Ï¿½':'ß', '±':'ß', 'Ã?Â¿Ã?Â':'ß', '³':'ä', '\u0015':'ß', '?=':'ü', 'Â':'', '\u0019':'ü', '\u001b':'ü', '\u001b':'', '':''},\n",
    "                'Wweden': {'Ï¿½':'å', 'Å':'å', '':'å', 'Ã?':'Ö', 'Ã¤':'Ä', 'Ã¶':'Ö'},\n",
    "                'Finland': {'¦':'P', '':'Å', '„':'ö'},\n",
    "                'France':{'':'É', },\n",
    "                'Italy': {'{':'À', '':''},\n",
    "                'Czech Republic': {'Ã?':'é', 'Ã':'í'}\n",
    "                }\n",
    "                \n",
    "valid_encodings = {\n",
    "                    'china': ['iso8859_5', 'gbk', 'euc_jis_2004',          'ptcp154', 'shift_jis', 'big5', 'utf_16'],\n",
    "                    'russia': ['iso8859_5' ],                    \n",
    "                   }                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def get_correct_text(text):\n",
    "    def get_correctly_encoded_word(decoding_result, valid_encodings):\n",
    "        res = [decoding_result[item] for item in valid_encodings if(decoding_result.get(item, None) != None)]\n",
    "        return None if len(res) == 0 else res[0]\n",
    "    word_list = [word for word in text.split(' ')]\n",
    "    for i, word in enumerate(word_list):\n",
    "        if(common.is_all_digit(word)):\n",
    "            continue\n",
    "        else:\n",
    "            if(is_valid_word(word)):\n",
    "                continue\n",
    "            else:\n",
    "                decoding_result = try_decoding2(word)\n",
    "\n",
    "                # if(len(decoding_result) == 0): continue\n",
    "                if(decoding_result.get('utf_8', None) != None): word_list[i] = decoding_result['utf_8']; continue\n",
    "\n",
    "                if('china' in word_list and get_correctly_encoded_word(decoding_result, valid_encodings['china']) != None):\n",
    "                    word_list[i] = get_correctly_encoded_word(decoding_result, valid_encodings['china']); continue\n",
    "\n",
    "                if('russia' in word_list and get_correctly_encoded_word(decoding_result, valid_encodings['russia']) != None):\n",
    "                    word_list[i] = get_correctly_encoded_word(decoding_result, valid_encodings['russia']); continue\n",
    "\n",
    "                if('cp858' in decoding_result.keys()): word_list[i] = decoding_result['cp858']; continue\n",
    "                if('cp852' in decoding_result.keys()): word_list[i] = decoding_result['cp852']; continue\n",
    "\n",
    "                new_word = ''\n",
    "                replacements_countries = replacements.keys()\n",
    "                for country in replacements_countries:\n",
    "                    if(country in word_list):\n",
    "                        print(country, word)\n",
    "                        new_word = word\n",
    "                        for k, v in replacements[country].items():\n",
    "                            new_word = new_word.replace(k, v)\n",
    "                        break\n",
    "                if(new_word != ''):\n",
    "                    word_list[i] = new_word\n",
    "                    continue\n",
    "\n",
    "                if('utf_16' in decoding_result.keys()): word_list[i] = decoding_result['utf_16']; continue\n",
    "                if('iso8859_5' in decoding_result.keys()): word_list[i] = decoding_result['iso8859_5']; continue\n",
    "                if('iso2022_jp_2' in decoding_result.keys()): word_list[i] = decoding_result['iso2022_jp_2']; continue\n",
    "    return ' '.join(word_list)\n",
    "\n",
    "# get_correct_text('INTERCONTINENTAL CHENGDU CENTU 88 SHIJICHENG ROAD, GAOXING DISTRICT, CHENGDU. 88 ZJ, CHENGDU/CHENGDU/Ï¿½É¶Ï¿½ CN China CNY ZJ')\n",
    "# get_correct_text('NH FURTH NURNBERG KÃ?Â¶NIGSTRASSE 140 NÃ?Â¼RNBERG FÃ?Â¼RTH DE Germany EUR NH')\n",
    "# get_correct_text('RADISSON BLU HOTEL, SAKARYA HANLIKÃ¶Y MAH. ESKISEHIR CAD 70 SAKARYA TR Turkey EUR ')\n",
    "# get_correct_text('MOTEL ONE INNSBRUCK 1 SÃ¼DBAHNSTRAÃ?E TOP 1 1 SÃ¼DBAHNSTRAÃ?E TOP 1 INNSBRUCK AT Austria EUR MOT')\n",
    "get_correct_text('HOTEL WYNDHAM AIRPORT STUTTGART WYNDHAM STUTTGART AIRPORT FLUGHAFENSTRAÃƒÅ¸E 51 70629 STUTTGART DE Germany EUR')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# refine hotels data\n",
    "\n",
    "item_count = {}\n",
    "\n",
    "for index, row in HotelObservation.iterrows():    \n",
    "    for col in row.values:\n",
    "        col = str(col)\n",
    "        if(common.is_all_digit(col)):\n",
    "            pass\n",
    "        else:\n",
    "            if(is_valid_word(col)):\n",
    "                pass\n",
    "            else:\n",
    "                country = str(row['countryname']).lower().strip()\n",
    "                decoding_result = try_decoding2(col)\n",
    "\n",
    "                if(len(decoding_result) == 0): continue                \n",
    "                if(decoding_result.get('utf_8', None) != None or decoding_result.get('utf_16', None) != None): continue\n",
    "\n",
    "                if(country in ['china', 'russia'] ): continue #['norway', 'china', 'russia', 'germany', 'sweden']\n",
    "                if('cp858' in decoding_result.keys()): continue\n",
    "                if('cp852' in decoding_result.keys()): continue\n",
    "\n",
    "                if(country in replacements.keys()):\n",
    "                    for k, v in replacements[country].items():\n",
    "                        col = col.replace(k, v)\n",
    "                if(is_valid_word(col)): continue\n",
    "\n",
    "                if('iso8859_5' in decoding_result.keys()): continue\n",
    "\n",
    "                if('iso2022_jp_2' in decoding_result.keys()): continue                \n",
    "\n",
    "                # get_valid_word(col)\n",
    "\n",
    "                print(country, '-----', col, '\\n', decoding_result)\n",
    "                print()\n",
    "                # is_valid = [1 for val_enc in valid_encodings[country] if(val_enc in decoding_result.keys())]\n",
    "                # if(sum(is_valid) > 0): continue\n",
    "\n",
    "                for val_enc in decoding_result.keys():\n",
    "                    if(val_enc not in item_count.keys()):\n",
    "                        item_count[val_enc] = 1\n",
    "                    else:\n",
    "                        item_count[val_enc] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_sort(item_count, sort_on='values', desc=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corrupted_string = \"LULEÏ¿½\"\n",
    "\n",
    "# Assume it was meant to be UTF-8 but was misinterpreted\n",
    "# First, we need to encode it back to bytes assuming it was misinterpreted as Windows-1252 or a similar encoding\n",
    "bytes_string = corrupted_string.encode('windows-1252')\n",
    "\n",
    "# Now, we decode it as UTF-8\n",
    "correct_string = bytes_string.decode('utf-8')\n",
    "\n",
    "print(correct_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xxx = \"[GUEST HOUSE] ÇÑ¸¶À½ \"\n",
    "\n",
    "print(is_valid_word(xxx), '\\n')\n",
    "\n",
    "print(try_decoding2(xxx))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ['utf_8', 'utf_16', 'mac_latin2'] 'shift_jis'\n",
    "# {'China', 'shift_jis}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xxx.encode('latin1').decode('utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for c in '''RH\\NPERLE BQD NEUSTADT''':\n",
    "    print( c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "repr(str('RH\\NPERLE BQD NEUSTADT'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "whenever there is an invalid word, we use try_decoding to get the right alternatives for that word. and then we make new insertion with the alternative/alternatives words and return the list.\n",
    "\n",
    "dump_into_sql: runs the query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_new_insertions(row):\n",
    "    columns = extract_values(row.split(' VALUES ')[1])\n",
    "    new_insertions = []\n",
    "    for col in columns:\n",
    "        if(not(is_valid_word(col))):\n",
    "            decoded_values = try_decoding(col)\n",
    "            if(len(decoded_values) > 0):\n",
    "                for decoded_value in decoded_values:\n",
    "                    new_insert = row\n",
    "                    new_insert = new_insert.replace(col, decoded_value)\n",
    "                    new_insertions.append(new_insert)\n",
    "    return new_insertions\n",
    "\n",
    "def dump_into_sql(insertions):\n",
    "    for insertion in insertions:\n",
    "        cursor.execute(insertion)\n",
    "        connection.commit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "list of available tables in world_db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "table_names = ['cityNames'] # ['locales', 'regions', 'regionNames', 'countries', 'countryNames', 'cities', "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for table in table_names:\n",
    "    rows = get_table_rows(table)\n",
    "    print(table, len(rows))\n",
    "    i = 0\n",
    "    # try:\n",
    "    for row in rows:\n",
    "        insertions = [row]\n",
    "        new_insertions = get_new_insertions(row)\n",
    "        if(len(new_insertions) > 0):\n",
    "            for ins in new_insertions:\n",
    "                insertions.append(ins)\n",
    "\n",
    "        i += 1\n",
    "        if(i % 100 == 0):\n",
    "            clear_output(wait=True)\n",
    "            display(f'Current row: {i}')\n",
    "\n",
    "        dump_into_sql(insertions)\n",
    "    # except Exception as e:\n",
    "        # print('error', e)\n",
    "        # print(insertions)\n",
    "        # print(new_insertions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec\n",
    "\n",
    "# Sample sentences\n",
    "sentences = [\n",
    "    [\"apple\", \"fruit\", \"tasty\"],\n",
    "    [\"banana\", \"fruit\", \"yellow\"],\n",
    "    [\"salmon\", \"fish\", \"pink\"],\n",
    "    [\"trout\", \"fish\", \"freshwater\"]\n",
    "]\n",
    "\n",
    "# Train a Word2Vec model\n",
    "model = Word2Vec(sentences, vector_size=15, window=2, min_count=1, workers=4)\n",
    "# model.save(\"word2vec_example.model\")\n",
    "\n",
    "# Find vector for a word\n",
    "vector_apple = model.wv['apple']\n",
    "\n",
    "# Find similar words\n",
    "similar_to_apple = model.wv.most_similar('apple', topn=3)\n",
    "\n",
    "print(similar_to_apple)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
